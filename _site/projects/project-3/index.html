<!DOCTYPE html>
<html>
  <head>
	<meta charset="utf-8">

	<title>Minghe Jiang Project Portfolio</title>
	<link rel="icon" type="image/png" href="/portfolio/public/images/msr-student-template-favicon.png">

	<link rel="stylesheet" href="/portfolio/public/stylesheets/style.css">

	<script src="//ajax.googleapis.com/ajax/libs/jquery/2.1.1/jquery.min.js"></script>
</head>

  <body>

    <div id="wrapper">

      <header>
    <nav>
    	<a href="/portfolio/"><h1>Minghe's Portfolio</h1></a>
    	<ul>
    		<li><a href="/portfolio/">Portfolio</a></li>
    		<li><a href="/portfolio/about/">About</a></li>
    		<li><a href="/portfolio/contact/">Contact</a></li>
    	</ul>
    </nav>
</header>


      <main class="project">
	<section id="contact-content">
		<img id="project-image" src="/portfolio//public/images/baxterdrawing.png">
		<h1 id="project-title">Baxter Drawing</h1>
		<h2 id="project-date">March 22, 2016</h2>

		<h2 id="overview">Overview</h2>

<h3 id="introduction">Introduction:</h3>

<p>The purpose of this project was to create a demonstration that combined robot arm kinematics and control with basic image processing. In the demo, a user draws a picture on a whiteboard and we use the right hand camera of a Baxter Research Robot to take an image of the user’s picture. Canny edge detection is used to build a set of SE(3) waypoints that represent the user’s picture. Finally, we solve the inverse kinematics for each waypoint, and design a joint space trajectory for Baxter’s left arm to follow. Baxter then draws a replica of the user’s original picture.</p>

<h3 id="files-in-package">Files In Package:</h3>
<ul>
  <li>
    <h4 id="in-package-drawingleft">In Package (drawing_left):</h4>

    <p><a href="https://github.com/MingheJiang/baxter_drawing/blob/master/takephoto_right/takephoto_right.py">takephoto_right.py</a></p>

    <blockquote>
      <blockquote>
        <p>This file firstly let the right camera find the content in the paper. Then after taking a picture of the content and appling image processing on it, its pixels converted into x,y locations in Baxter workspace finally.</p>
      </blockquote>
    </blockquote>

    <p><a href="https://github.com/MingheJiang/baxter_drawing/blob/master/drawing_left/setup.dat">setup.dat</a></p>

    <blockquote>
      <blockquote>
        <p>Setup right arm and distance.</p>
      </blockquote>
    </blockquote>
  </li>
  <li>
    <h4 id="in-package-drawingleft-1">In Package (drawing_left):</h4>

    <p><a href="https://github.com/MingheJiang/baxter_drawing/blob/master/drawing_left/joint_trajectory_action_server.py">joint_trajectory_action_server.py</a></p>

    <p><a href="https://github.com/MingheJiang/baxter_drawing/blob/master/drawing_left/joint_trajectory_client.py">joint_trajectory_client.py</a></p>

    <blockquote>
      <blockquote>
        <p>This file reads x,y,z data from the csv file which exported from takephoto_right.py and converts these locations into baxter 7 joint angles by using <code class="highlighter-rouge">baxter_ik_move()</code>. Then these sets of 7 joint angles are put into joint trajectory one by one. The trajectory is a function of time. Baxter will follow this trajectory to draw the content of picture.</p>
      </blockquote>
    </blockquote>

    <p><a href="https://github.com/MingheJiang/baxter_drawing/blob/master/takephoto_right/setup.dat">setup.dat</a></p>

    <blockquote>
      <blockquote>
        <p>Setup left arm and distance.</p>
      </blockquote>
    </blockquote>
  </li>
</ul>

<h3 id="image-processing">Image Processing:</h3>

<p>The right camera firstly gets the canny image of its vision by <code class="highlighter-rouge">cv2.Canny()</code> and then finds the location of the content in paper by finding and moving to the x,y location of the center of its contour rectangle several times with <code class="highlighter-rouge">cv2.findContours()</code> and <code class="highlighter-rouge">cv2.rectangle()</code>.</p>

<p>After finding the content and taking the canny image of it, the image is dilated and eroded in order to get the better result for houghing lines. By implementing <code class="highlighter-rouge">cv2.HoughLinesP()</code>, the sets of x,y pixels of these lines is obtained.</p>

<p>Then the new x,y locations are splined by the function  <code class="highlighter-rouge">interpolate.splev()</code> to get much smoother lines.</p>

<p align="center">
<iframe width="560" height="315" src="https://www.youtube.com/embed/1d311kzw10Y" frameborder="0" allowfullscreen=""></iframe>
</p>


	</section>

</main>


      <footer>
    <ul>
    	<li>minghej@u.northwestern.edu</li>
    	<li>1310 Hinman Ave, Evanston, IL, 60201</li>
    	<li>(773) 273-0198</li>
    </ul>
</footer>



    </div>

  </body>
</html>